{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/responsible_ai/privacy/tutorials/privacy_report\n",
    "\n",
    "https://github.com/tensorflow/privacy/tree/master/tensorflow_privacy/privacy/privacy_tests/membership_inference_attack"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I train a new approximant layer as substitute for final sigmoid layer  that is trained on input-output dataset of the output softmax layer of original model. This trained new approximant layer is than substituted in the original model to form refined model\n",
    "The output vec is one hot encoded output of layer\n",
    "\n",
    "The refined model is trained again with loss as MSE.\n",
    "\n",
    "I train this reference layer using MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 08:27:57.242918: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-19 08:27:58.064712: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the neural network architecture\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.reshape=nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.nonlinear = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.reshape(x)\n",
    "        # x = torch.flatten(x, start_dim=1)\n",
    "        # x = x.reshape(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x= self.nonlinear(x)\n",
    "        # x = torch.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "train_data = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST('data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64*20, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64*20, shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MNISTClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNISTClassifier(\n",
       "  (reshape): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (nonlinear): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy before Linearization is:  0.8816\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "in_preds = []\n",
    "in_label = []\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        # inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        #load outputs to cpu\n",
    "        outputs = outputs.cpu()\n",
    "        in_preds.append(outputs)\n",
    "        in_label.append(labels)\n",
    "    in_preds = torch.cat(in_preds)\n",
    "    in_label = torch.cat(in_label)\n",
    "print(\n",
    "    \"Test Accuracy before Linearization is: \",\n",
    "    accuracy_score(np.array(torch.argmax(in_preds, axis=1)), np.array(in_label)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 109,386 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# count the number of trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNISTClassifier(\n",
       "  (reshape): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (nonlinear): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNISTClassifier(\n",
       "  (reshape): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (nonlinear): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataset for linearization\n",
    "in_vec=[]\n",
    "out_vec=[]\n",
    "\n",
    "second_last_output = None\n",
    "\n",
    "def hook(module, input, output):\n",
    "    global second_last_output\n",
    "    second_last_output = output\n",
    "\n",
    "# Register the hook to the second last layer (e.g., the `fc` layer in this example)\n",
    "handle = model.fc3.register_forward_hook(hook)\n",
    "\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(images)\n",
    "    pre_output = second_last_output\n",
    "    np_output = output.cpu().detach().numpy()\n",
    "    np_pre_output = pre_output.cpu().detach().numpy()\n",
    "    # cnvert to torch\n",
    "    np_output = torch.from_numpy(np_output)\n",
    "    np_pre_output = torch.from_numpy(np_pre_output)\n",
    "    in_vec.append(np_output)\n",
    "    out_vec.append(np_pre_output)\n",
    "\n",
    "in_vec = torch.cat(in_vec)\n",
    "out_vec = torch.cat(out_vec)\n",
    "\n",
    "handle.remove()\n",
    "\n",
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Approximant(\n",
       "  (fc1): Linear(in_features=10, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "# Create a PyTorch dataset\n",
    "layer_dataset = TensorDataset(in_vec, out_vec)\n",
    "\n",
    "# Create a PyTorch DataLoader\n",
    "batch_size = 64*20  # or any other batch size you prefer\n",
    "layer_train_loader = DataLoader(layer_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Approximant(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Approximant, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 10)\n",
    "        # self.minnonlinear = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        # x = self.minnonlinear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "linearized_proxy=Approximant().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(linearized_proxy.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "\n",
    "# train the proxy linear layer\n",
    "for epoch in range(epochs):\n",
    "    for inputs, outputs in layer_train_loader:\n",
    "        inputs, outputs = inputs.to(device), outputs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred_outputs = linearized_proxy(inputs)\n",
    "        loss = criterion(pred_outputs.to(device),outputs.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "linearized_proxy.to('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackedModel(\n",
       "  (stacked_model): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=128, bias=True)\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): Linear(in_features=64, out_features=10, bias=True)\n",
       "    (4): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super().__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1= self.model1(x)\n",
    "        x = self.model2(x1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class StackedModel(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super(StackedModel, self).__init__()\n",
    "        self.model1_layers = model1.children()\n",
    "        self.model2_layers = model2.children()\n",
    "        \n",
    "        # Concatenate layers from both models\n",
    "        stacked_layers = []\n",
    "        for layer in self.model1_layers:\n",
    "            stacked_layers.append(layer)\n",
    "        for layer in self.model2_layers:\n",
    "            stacked_layers.append(layer)\n",
    "        \n",
    "        self.stacked_model = nn.Sequential(*stacked_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.stacked_model(x)\n",
    "\n",
    "\n",
    "\n",
    "# The proxy linear model\n",
    "model2 = linearized_proxy\n",
    "\n",
    "# Remove the last layer of the original model\n",
    "model1 = nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "\n",
    "# Combine the amputed original model and proxy linear model\n",
    "\n",
    "# combined_model = CombinedModel(model1, model2)\n",
    "combined_model = StackedModel(model1, model2)\n",
    "\n",
    "\n",
    "combined_model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The modified model has 109,496 trainable parameters\n",
      "Test Accuracy after Linearization is:  0.176\n"
     ]
    }
   ],
   "source": [
    "#------------------------------train-------------------------------------\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(combined_model.parameters(), lr=1e-5)\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "# train the proxy linear layer\n",
    "for epoch in range(epochs):\n",
    "    for inputs, outputs in train_loader:\n",
    "        #One hot encode outputs\n",
    "        outputs = torch.nn.functional.one_hot(outputs, num_classes=10).float()\n",
    "        inputs, outputs = inputs.to(device), outputs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred_outputs = combined_model(inputs)\n",
    "        #convert to float\n",
    "        pred_outputs = pred_outputs.float()\n",
    "\n",
    "        loss = criterion(pred_outputs.to(device),outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# count the number of trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The modified model has {count_parameters(combined_model):,} trainable parameters')\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "in_preds = []\n",
    "in_label = []\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = combined_model(inputs)\n",
    "        #load outputs to cpu\n",
    "        outputs = outputs.detach().cpu()\n",
    "        in_preds.append(outputs)\n",
    "        in_label.append(labels)\n",
    "    in_preds = torch.cat(in_preds)\n",
    "    in_label = torch.cat(in_label)\n",
    "print(\n",
    "    \"Test Accuracy after Linearization is: \",\n",
    "    accuracy_score(np.array(torch.argmax(in_preds, axis=1)), np.array(in_label)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = combined_model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackedModel(\n",
       "  (stacked_model): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=128, bias=True)\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): Linear(in_features=64, out_features=10, bias=True)\n",
       "    (4): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference latency: 76.68638229370117187500 micro-seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "input_tensor=torch.randn((1, 1, 28, 28))\n",
    "\n",
    "model.eval()\n",
    "# Perform a warm-up run to avoid potential overhead caused by initial device setup\n",
    "with torch.no_grad():\n",
    "    _ = target_model(input_tensor)\n",
    "\n",
    "\n",
    "\n",
    "num_iterations = 1000  # Choose a suitable number of iterations to average over\n",
    "\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_iterations):\n",
    "        _ = target_model(input_tensor)\n",
    "end_time = time.time()\n",
    "\n",
    "inference_latency = (end_time - start_time) / num_iterations\n",
    "#convert to microseconds\n",
    "inference_latency = inference_latency * 1000000\n",
    "print(f'Inference latency: {inference_latency:.20f} micro-seconds')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subbase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
